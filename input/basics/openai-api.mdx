---
sidebar_label: OpenAI API
---

# Изучаем OpenAI API

Понимание работы с API OpenAI позволит вам без проблем понимать трудоустройство фреймворков, которые мы будем проходить в блоке Junior, а также пригодится при отладке проблем и работы со всеми OpenAI-совместимыми провайдерами.

## Questions

- Как выглядит тело API запроса в OpenAI API?
- Какие параметры можно настраивать кроме temperature, max_tokens?
- Как работает потоковая передача данных в OpenAI API?
- Как использовать функциональные вызовы (function calling) в чат-моделях?
- Что такое структурированный вывод?

## Steps

### 1. Читаем документацию [OpenAI API](https://platform.openai.com/docs/api-reference)

Читаем всё про ручку `chat/completions`, ничего не пропускаем. Весь будущий материал будет строиться на этой ручке и вам важно в деталях понимать, как она работает, иначе вы будете путаться.

### 2. Читаем странички из Cookbook

1. [Как считать токены в тексте?](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)

2. [Как выглядят чанки в потоковом ответе опенаи? Какая информация передается в первом, промежуточных и последнем чанке?](https://cookbook.openai.com/examples/how_to_stream_completions)

3. [По функциям, читаем целиком (ru)](https://www.ai-cookbook.ru/docs/guides/tools_yc/FC_why), 3 главы. *Не обращайте внимание на то, что там написано про YandexGPT, код работает с API формата OpenAI.*

Аналог - [how_to_call_functions_with_chat_models](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models)

4. [Как использовать structured outputs?](https://cookbook.openai.com/examples/how_to_use_structured_outputs) или альтернативный курс на час на [deeplearning.ai](https://www.deeplearning.ai/short-courses/getting-structured-llm-output/)

## Extra Steps

### E1. Закладываем принцип разделяй и властвуй.

https://cookbook.openai.com/examples/entity_extraction_for_long_documents

### E2. Каких параметров пока не существует ни в одном public API?

*\<EOS> - end of sequence - токен, после которого мы обычно перестаем производить предсказания следущих токенов*

Хочу немного интересных моментов вам преоткрыть, которые я не встречал пока у провайдеров. Обычно температуру и все другие параметры мы задаем константой - но также это можно делать и **динамически** - чтобы параметры изменялись в зависимости от условий, математических функций и тд. Примеры:
1. Например, пока LLM анализирует задачу -  держим температуру на нуле; а когда перейдет к написанию творческого текста - поднимем до 0.6
2. После того, как LLM генерирует \<EOS> - мы можем продолжить генерировать токены
3. Мы можем мануально умножать вероятность вывода \<EOS> на любое значение (даже на 0) до определенного количества сгенерированных токенов. Кроме умножения доступна любая логика.
4. Также как с \<EOS>, мы можем поступать вообще с любыми токенами - можем придумывать любую логику которая только родится в шизе. Можем статически/динамически/логически влиять на вероятности (0-100%) генерации определенного токена или набора токенов или последовательности токенов.

Что можно было бы добавить в существующие API:
- возможность запрещать генерировать \<EOS> при длине последовательности меньше определенного количества токенов (MinOutputLen)
- BannedTokensToGenerate
- возможность управлять вероятностью \<EOS> в зависимости от длины сгенерированного текста (BoostEos)
- NGramPenalty
- возможность продолжать генерацию какое-то количество шагов даже после \<EOS> (IncludeLateHypotheses)
- запрещать генерировать определенные токены после какого-то количества повторений (MaxRepeats)
- возможность выбирать алгоритм семплинга через SamplerParams (Sampling, BeamSearch, StochasticBeamSearch), NumHypos & BeamSize
- возможность изменять температуру в процессе генерации - TemperatureScheduler

Для ризонинг-моделей можно умножить все эти фичи на два: для фазы размышлений и генерации.

##### Конечно, всё это можно реализовать при локальном инференсе модели на своём железе. Приходите работать в OpenAI/Sber/Yandex/Anthropic - там все эти параметры доступны в private API!

## Now we know...

Мы изучили основные компоненты OpenAI API и научились:
- Подсчитывать токены в текстах с помощью tiktoken для оптимизации запросов
- Работать с потоковыми ответами для создания более отзывчивых интерфейсов
- Использовать функциональные вызовы (function calling) для структурированного взаимодействия с внешними инструментами
- Получать структурированные ответы в JSON формате для удобной обработки
- Применять принцип "разделяй и властвуй" для обработки длинных документов

Эти навыки являются фундаментальными при создании AI Агентов, поскольку позволяют эффективно управлять коммуникацией между агентом и LLM, а также обеспечивают возможность интеграции с внешними системами.

## Exercises

1. Какие есть роли у сообщений?

2. Вы хотите отправить в LLM два запроса с сохранением контекста:
    1. "Привет! Меня зовут Иван."
    2. "Как меня зовут?"

    Как будет выглядеть тело второго запроса в модель?

3. Придумайте три примера использования функциональных вызовов, которые могли бы существенно улучшить возможности AI Агента.

![Ask ChatGPT](https://img.shields.io/badge/Ask%20ChatGPT-8A2BE2?style=for-the-badge)

4. **Практическое задание:**

Создайте простой скрипт на Python, который будет выводить в консоль только первый, второй, предпоследний и последний чанки ответа от модели.
